<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml"  lang="en">
<head>
<meta charset="UTF-8" />
<meta name="viewport" content="width=device-width, initial-scale=1"/>

<title>How to build a simple artificial neural network with Go | sausheong&#39;s space</title>

<meta property='og:title' content='How to build a simple artificial neural network with Go - sausheong&#39;s space'>
<meta property='og:description' content='I have written a lot of computer programs in my career, most of the time to solve various problems or perform some tasks (or sometimes just for fun). For most part, other than bugs, as long as I tell the computer what to do very clearly (in whichever the programming language I use) it will obediently follow my instructions.
This is because computer programs are really good at executing algorithms &ndash; instructions that follow defined steps and patterns that are precise and often repetitious.'>
<meta property='og:url' content='https://sausheong.github.io/posts/how-to-build-a-simple-artificial-neural-network-with-go/'>
<meta property='og:site_name' content='sausheong&#39;s space'>
<meta property='og:type' content='article'><meta property='og:image' content='https://www.gravatar.com/avatar/ee191858f0d96ad93098694537f71998?s=256'><meta property='article:author' content='https://facebook.com/sausheong'><meta property='article:section' content='Posts'><meta property='article:published_time' content='2018-03-24T01:51:07&#43;08:00'/><meta property='article:modified_time' content='2018-03-24T01:51:07&#43;08:00'/><meta name='twitter:card' content='summary'><meta name='twitter:site' content='@sausheong'><meta name='twitter:creator' content='@sausheong'>
<link rel="stylesheet" href="https://sausheong.github.io/css/style.css"/><link rel='stylesheet' href='/css/custom.css'></head>
<body>

<section class="section">
  <div class="container">
    <nav class="nav">
      <div class="nav-left">
        <a class="nav-item" href="https://sausheong.github.io"><h1 class="title is-4">sausheong&#39;s space</h1></a>
      </div>
      <div class="nav-right">
        <nav class="nav-item level is-mobile"><a class="level-item" href='mailto:sausheong@gmail.com' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M4 4h16c1.1 0 2 .9 2 2v12c0 1.1-.9 2-2 2H4c-1.1 0-2-.9-2-2V6c0-1.1.9-2 2-2z"/>
    <polyline points="22,6 12,13 2,6"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://facebook.com/sausheong' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M18 2h-3a5 5 0 0 0-5 5v3H7v4h3v8h4v-8h3l1-4h-4V7a1 1 0 0 1 1-1h3z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://github.com/sausheong' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://instagram.com/sausheong' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <rect x="2" y="2" width="20" height="20" rx="5" ry="5"/>
    <path d="M16 11.37A4 4 0 1 1 12.63 8 4 4 0 0 1 16 11.37z"/>
    <line x1="17.5" y1="6.5" x2="17.5" y2="6.5"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://linkedin.com/in/sausheong' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path stroke-width="1.8" d="m5.839218,4.101561c0,1.211972 -0.974141,2.194011 -2.176459,2.194011s-2.176459,-0.982039 -2.176459,-2.194011c0,-1.211094 0.974141,-2.194011 2.176459,-2.194011s2.176459,0.982917 2.176459,2.194011zm0.017552,3.94922l-4.388022,0l0,14.04167l4.388022,0l0,-14.04167zm7.005038,0l-4.359939,0l0,14.04167l4.360816,0l0,-7.370999c0,-4.098413 5.291077,-4.433657 5.291077,0l0,7.370999l4.377491,0l0,-8.89101c0,-6.915523 -7.829986,-6.66365 -9.669445,-3.259423l0,-1.891237z"/>
    
  </svg></i>
            </span>
          </a><a class="level-item" href='https://twitter.com/sausheong' target='_blank' rel='noopener'>
            <span class="icon">
              <i class><svg viewbox='0 0 24 24' stroke-linecap='round' stroke-linejoin='round' stroke-width='2' aria-hidden='true'>
    
    <path d="M23 3a10.9 10.9 0 0 1-3.14 1.53 4.48 4.48 0 0 0-7.86 3v1A10.66 10.66 0 0 1 3 4s-4 9 5 13a11.64 11.64 0 0 1-7 2c9 5 20 0 20-11.5a4.5 4.5 0 0 0-.08-.83A7.72 7.72 0 0 0 23 3z"/>
    
  </svg></i>
            </span>
          </a></nav>
      </div>
    </nav>
  </div>
</section>

<section class="section">
  <div class="container">
    <div class="subtitle is-6 is-pulled-right">
      
    </div>
    <h2 class="subtitle is-6">March 24, 2018</h2>
    <h1 class="title">How to build a simple artificial neural network with Go</h1>
      
    <div class="content">
      

<p>I have written a lot of computer programs in my career, most of the time to solve various problems or perform some tasks (or sometimes just for fun). For most part, other than bugs, as long as I tell the computer what to do very clearly (in whichever the programming language I use) it will obediently follow my instructions.</p>

<p>This is because computer programs are really good at executing algorithms &ndash; instructions that follow defined steps and patterns that are precise and often repetitious. And in most cases they work well for us for tasks like number crunching or repetitious boring work.</p>


<figure >
    
        <img src="https://upload.wikimedia.org/wikipedia/commons/4/4e/Eniac.jpg" width="600px" />
    
    
    <figcaption>
        <h4>The ENIAC was one of the first general-purpose, programmable computers ever made</h4>
        
    </figcaption>
    
</figure>


<p>What computer programs are not so good at doing though, are tasks that are not so well defined, and doesnâ€™t follow precise patterns.</p>


<figure >
    
        <img src="https://imgs.xkcd.com/comics/tasks.png" width="250px" />
    
    
    <figcaption>
        <h4>In the 60s, Marvin Minsky assigned a couple of undergrads to spend the summer programming a computer to use a camera to identify objects in a scene. He figured they&#39;d have the problem solved by the end of the summer. Half a century later, we&#39;re still working on it.</h4>
        
    </figcaption>
    
</figure>


<p>So how can we use computers to do such tasks? Just think about how <em>you</em> do this task. You probably learned about birds when you&rsquo;re young, and you&rsquo;ve been told certain animals are birds and certain animals are not, mostly through seeing them either in real life or through picture books. When you get it wrong, you&rsquo;ll be told and you remember that. Over time you have a <em>mental model</em> of what&rsquo;s a bird and what&rsquo;s not. Every time you see something parts of a bird (clawed feet, feathered wings, sharp beak) you don&rsquo;t even need to see the whole animal anymore, you&rsquo;ll automatically identify it correctly by comparing it with your mental model.</p>

<p>So how do we do this with computer programs? Basically we do the same thing. We try to create a <em>model</em> that we can use to compare inputs with, through a trial and error process. And since computer programs are all mathematics, you can guess that it&rsquo;s going to be a <em>mathematical model</em> that we&rsquo;re going to be talking about.</p>

<h1 id="a-guessing-game">A guessing game</h1>

<p>Let&rsquo;s take a simple example create a black box that accepts an input and tries to predict the output.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/predictor1.png" width="400px" />
    
    
    <figcaption>
        <h4>A simple predictor</h4>
        
    </figcaption>
    
</figure>


<p>We feed it with an input and get the output from this predictor. Since we know what the actual output should be, we can tell how different the predicted output is from the actual output. This difference between the actual and the predicted output becomes the <em>error</em>.</p>

<p>Of course, if the predictor is static and can&rsquo;t be changed, it&rsquo;s all pretty much moot. When we feed our predictor with an input, an output is produced with an error and that&rsquo;s the end of the story. Not very useful.</p>

<p>To make our predictor more useful let&rsquo;s give it a configurable parameter  that we can use to influence the output. Since it only predicts correctly if there is no error, we want to change the parameter such that the error shrinks as we keep on feeding the predictor with data. The aim is to get a predictor that predicts the correct output most of the time, without actually needing to give clear instructions to the predictor.</p>

<p>In other words, this is very much like a numbers guessing game.</p>

<p>Letâ€™s see this in a more practical way. Letâ€™s say we have a predictor with the simple mathematical formula <code>o = i x c</code> where <code>o</code> is the output, <code>i</code> is the input and <code>c</code> is configurable parameter.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/predictor2.png" width="400px" />
    
    
    <figcaption>
        <h4>A simple predictor with a configurable parameter</h4>
        
    </figcaption>
    
</figure>


<p>We are also given a confirmed valid output with a given input, that is, we know if <code>i</code> is 10, <code>o</code> is 26. How do we find <code>c</code> using the predictor?</p>

<p>First, we need take a random prediction, let&rsquo;s say <code>c</code> is 2. Let&rsquo;s put in the input 10, and crank up the predictor. The output <code>o</code> is 20. Since the error <code>e = t - o</code> where <code>t</code> is the truth (or target), this means <code>e = 26 - 20 = 6</code>. Our error <code>e</code> is 6 and we want to achieve 0, so let&rsquo;s try again.</p>

<p>Let&rsquo;s make <code>c</code> to be 3. The output is then <code>30</code> and is <code>e</code> is now <code>-4</code>. Oops, we overshot! Let&rsquo;s go back a bit and make <code>c</code> to be 2.5. That makes <code>o</code> to be 25, and <code>e</code> to be 1. Finally we try <code>c</code> to be 2.6 and we get the error <code>e</code> to be 0!</p>

<p>Once we know what <code>c</code> is, we can use the predictor to predict the output for other inputs. Let&rsquo;s say the input <code>i</code> is now 20, then we can predict  <code>o</code> to be 52.</p>

<p>As you can see, this method tries to find answers iteratively and improving itself as it goes along, until we get the best fit. This in essence is what <a href="[A Beginnerâ€™s Guide to AI/ML ðŸ¤–ðŸ‘¶ â€“ Machine Learning for Humans â€“ Medium](https://medium.com/machine-learning-for-humans/why-machine-learning-matters-6164faf1df12)" target="_blank">machine learning</a> is. The computer program  tries to find answers iteratively and â€˜learnsâ€™ through its mistakes until it achieves a model that can produce the best answer. Once it has the correct model, we can use the model to correctly guess the answers. This is very similar to what we humans do (by learning from past mistakes and correcting ourselves) but how exactly do we do it?</p>

<h1 id="how-humans-do-it">How humans do it</h1>

<p>Letâ€™s step out a bit. We talked a bit about how a machine can possible learn using mathematical functions. How humans do the same thing (as research over the years have shown) is using something called a <a href="[Understanding Neuronsâ€™ Role in the Nervous System](https://www.verywellmind.com/what-is-a-neuron-2794890)" target="_blank"><em>neuron</em></a>.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/purkinjecell.jpg" width="400px" />
    
    
    <figcaption>
        <h4>Drawing of neurons in the pigeon cerebellum, by Spanish neuroscientist Santiago RamÃ³n y Cajal in 1899</h4>
        
    </figcaption>
    
</figure>


<p>A neuron, or a nerve cell, is a cell that receives information, processes them and transmits through electrical and chemical signals. Our brain and spinal cord (part of what is called our central nervous system) consists of neurons.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/bneuron.png" width="600px" />
    
    
    <figcaption>
        <h4>A neuron with dendrites, a cell body and an axon</h4>
        
    </figcaption>
    
</figure>


<p>A neuron consists of a cell body, dendrites and an axon and can connected to each other to form neural networks. In a neural network, a neuron&rsquo;s axon is connected to the next neuron&rsquo;s dendrites and synaptic signals are transmitted from a neuron through its axon, and received by the next neuron through its dendrites. The connections between the axon and the dendrites is the synapse.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/synapses.png" width="600px" />
    
    
    <figcaption>
        <h4>Synapses are the connections between neurons</h4>
        
    </figcaption>
    
</figure>


<p>The incoming signals through the dendrite are strengthened or weakened based on how often the synaptic connections are being used and these strengthened or weakened signals are pooled together in the cell body.</p>

<p>If the pooled signals that are received are strong enough, it will trigger  a new signal that is sent down the line through the axon to other neurons.</p>

<p>As you can see, how a neuron work is somewhat analogous to our predictor earlier. It has a bunch of inputs through its dendrites which it processes and an output through its axon. Instead of a configurable parameter, each input is paired with the strength (or weight) of the synaptic connection.</p>

<p>With this information, letâ€™s get back into our predictor and make some changes.</p>

<h1 id="artificial-neurons">Artificial neurons</h1>

<p>We start off with building an artificial neuron that emulates the actual biological neuron. This artificial neuron is our upgraded predictor.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/aneuron1.png" width="400px" />
    
    
    <figcaption>
        <h4>An artificial neuron mimicking a biological one</h4>
        
    </figcaption>
    
</figure>


<p>Instead of a single input we have a bunch of inputs, each of the modified with a weight (in place of a configurable parameter). These modified inputs are summed up and passed through a triggering or <a href="[Understanding Activation Functions in Neural Networks](https://medium.com/the-theory-of-everything/understanding-activation-functions-in-neural-networks-9491262884e0)" target="_blank">activation function</a> which determines if an output should be sent.</p>

<p>So why an activation function (beyond the fact that a biological neuron behaves the same way)? There are a few good reasons but one the most important is that activation functions introduce non-linearity into the network. A neural network without activation functions (or a linear activation function) is basically just a <a href="[Introduction to Linear Regression](http://onlinestatbook.com/2/regression/intro.html)" target="_blank">linear regression</a> model and is not able to do more complicated tasks such as language translations and image classifications. You will see later how non-linear activation functions enable back propagation.</p>

<p>For now, we will assume the use of a common activation function, the <a href="[Sigmoid function](https://ipfs.io/ipfs/QmXoypizjW3WknFiJnKLwHCnL72vedxjQkDDP1mXWo6uco/wiki/Sigmoid_function.html)" target="_blank">sigmoid function</a>.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/sigmoid.png" width="600px" />
    
    
    <figcaption>
        <h4>Sigmoid function</h4>
        
    </figcaption>
    
</figure>


<p>The interesting thing to note about this function is that the output is always within the range between 0 and 1 but never reaches either.</p>

<h1 id="artificial-neural-networks">Artificial neural networks</h1>

<p>Just like we had neurons forming neural networks, we can also connect our artificial neurons to form artificial neural networks.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/ann.png" width="600px" />
    
    
    <figcaption>
        <h4>Artificial neural network with 3 layers</h4>
        
    </figcaption>
    
</figure>


<p>It seems quite a bit more complicated now!</p>

<p>However weâ€™re simply stacking the neurons up in different layers. All the inputs come in through the input layer, which sends its output to the hidden layer, which in turn sends its outputs to the final output layer. While the output from each node is the same (there is only 1 output) but the connections to the neurons in the next layer are weighted differently. For example, the inputs to the first node in the hidden layer would be <code>(w11 x i1) + (w21 x i2)</code>.</p>

<h1 id="simplifying-with-matrices">Simplifying with matrices</h1>

<p>Calculating the final outputs in this network can be a bit tedious if we have to do it one at a time, especially if we have a lot of neurons. Fortunately thereâ€™s an easier way. If we represent the inputs as well as the weights as matrices, we can use the matrix operations to make the calculations easier. In fact we donâ€™t need to do individual neuron input summation and output activation anymore, we simply do it layer by layer.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/matrix1.png" width="600px" />
    
    
    <figcaption>
        <h4>Using matrices</h4>
        
    </figcaption>
    
</figure>


<p>This will help a lot later in the code, as you will see.</p>

<p>We used the <a href="[How to Multiply Matrices](https://www.mathsisfun.com/algebra/matrix-multiplying.html)" target="_blank">matrix dot product</a> to handle the multiplication and summation of the input and weights, but for the activation function we will need to apply the sigmoid function to each of the matrix elements. Weâ€™ll have to do the same for every hidden layer, and also the output layer.</p>

<h1 id="tweaking-the-weights">Tweaking the weights</h1>

<p>You might realise at this point in time, our neural network is (conceptually) simply a larger version of the neuron, and therefore is very much like our predictor earlier. And just like our predictor we want to train our neural network to learn from its mistakes by passing it input with known output.  Then using the difference (error) between the known and actual outputs, we change the weights to minimise the error.</p>

<p>However youâ€™d probably realise that the neural network is quite a bit more complicated than our predictor. First, we have multiple neurons arranged in layers. As a result, while we know the final target output,  we donâ€™t know the intermediate target outputs of the different layers in between. Secondly, we while our predictor is linear, our neurons are passed through a non-linear activation function so the output is not linear. So how do we change the weights of the different connections?</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/aneuron2.png" width="400px" />
    
    
    <figcaption>
        <h4>Weights and outputs in artificial neuron</h4>
        
    </figcaption>
    
</figure>


<p>We know from our predictor earlier, we are looking to minimise the final output error <code>Ek</code> by changing the various output weights that connect between the hidden layer to the output layer, <code>wjk</code>.</p>

<p>Thatâ€™s very well and good but how do we minimise a value of a function by changing its input variable?</p>

<p>Letâ€™s look at this from a different perspective. We know the final output error <code>Ek</code>  is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/errork1.png" width="200px" />
    
    
</figure>


<p>However just subtracting <code>ok</code> from <code>tk</code> isnâ€™t a great idea because it will often result in negative numbers. If we are trying to find the final output error of the network, weâ€™re actually adding up all the errors, so if some of them are negative numbers it will result in the wrong final output error. A common solution is to use the <em>squared error</em>, which the name suggests is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/errork.png" width="250px" />
    
    
</figure>


<p>At the same time we know:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/ok.png" width="250px" />
    
    
</figure>


<p>So we know (roughly speaking) if we map <code>Ek</code> with <code>wjk</code> we will get a range of values (blue line) plotted on a chart (actually this is a multi- dimensional chart but in order to keep our collective sanity, Iâ€™m going to use a 2 dimensional chart):</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/g.png" width="400px" />
    
    
    <figcaption>
        <h4>Charting final output error to weights</h4>
        
    </figcaption>
    
</figure>


<p>As you can see, to reach the minimal value of <code>Ek</code> we follow the gradient downwards or the negative gradient. In other words, we try to find the negative gradient, change the weight accordingly, then find negative gradient again, until we reach the minimal point of <code>Ek</code>. This algorithm is called <a href="[An Introduction to Gradient Descent and Linear Regression](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/)" target="_blank"><em>gradient descent</em></a>.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/gd.png" width="400px" />
    
    
    <figcaption>
        <h4>Gradient descent</h4>
        
    </figcaption>
    
</figure>


<p>You might remember from secondary school calculus, in order to find the gradient of a point in a function we use <a href="[Introduction to Derivatives](https://www.mathsisfun.com/calculus/derivatives-introduction.html)" target="_blank">differentiation</a> to get the derivative of the function. This allows us to find out how much we need to tweak <code>wjk</code>. To find the minimum value of <code>Ek</code>, we subtract this amount from <code>wjk</code> and we do this repeatedly.</p>

<p>Letâ€™s do the maths.</p>

<p>In order to calculate the change that we need for the output weights <code>wjk</code> we should compute the derivative of the final output error <code>Ek</code> with respect to the output weights <code>wjk</code>. This means:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d1.png" width="200px" />
    
    
</figure>


<p>Thatâ€™s neat but how do we get our results using the other variables we have? To do that we need to use the <a href="[Chain rule - Wikipedia](https://en.wikipedia.org/wiki/Chain_rule)" target="_blank"><em>chain rule</em></a>:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d2.png" width="300px" />
    
    
</figure>


<p>That looks slightly better, but we can go even one step further:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d3.png" width="400px" />
    
    
</figure>


<p>Letâ€™s get to work. First, we need to find the derivative of <code>Ek</code> with respect to the final output <code>ok</code>.</p>

<p>From earlier, we know <code>Ek</code> is the squared error:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/errork.png" width="250px" />
    
    
</figure>


<p>But to differentiate nicely we scale it down by half (I know itâ€™s a bit like cheating but it makes our lives easier):</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d4.png" width="300px" />
    
    
</figure>


<p>The derivative of this is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d5.png" width="300px" />
    
    
</figure>


<p>Thatâ€™s easy enough! Letâ€™s look at the derivative of the final output <code>ok</code> with respect to the summation of the product of the intermediate outputs and the weights, <code>sumk</code>. We know the summation is passed through a sigmoid function <code>sig</code> in order to get the final output <code>ok</code>:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d6.png" width="200px" />
    
    
</figure>


<p>Therefore the derivative of the final output <code>ok</code> with respect to the summation <code>sumk</code>is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d7.png" width="400px" />
    
    
</figure>


<p>This is because we know the derivative of a sigmoid is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/dsigmoid.png" width="400px" />
    
    
</figure>


<p>I mentioned earlier above that there are good reasons why we use a sigmoid function â€” easy differentiation is one of them! The proof of this can be found <a href="[How to Compute the Derivative of a Sigmoid Function (fully worked example) - kawahara.ca](http://kawahara.ca/how-to-compute-the-derivative-of-a-sigmoid-function-fully-worked-example/)" target="_blank">here</a>.  Now since:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d6.png" width="200px" />
    
    
</figure>


<p>we can further simplify the equation to:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d8.png" width="350px" />
    
    
</figure>


<p>Finally we want to find the derivative of the summation <code>sumk</code> with respect to the output weight <code>wjk</code>.  We know the summation is the sum of the product of the output weight <code>wjk</code> and the previous output <code>oj</code>:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d9.png" width="300px" />
    
    
</figure>


<p>So the derivative of the summation <code>sumk</code> with respect to the output weight <code>wjk</code> is:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d10.png" width="200px" />
    
    
</figure>


<p>Now that we have all the 3 derivatives, letâ€™s put them together. Earlier, we said:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d3.png" width="400px" />
    
    
</figure>


<p>Therefore:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/d11.png" width="500px" />
    
    
</figure>


<p>With that we have the equation to change the weights for the output layer. What about the weights for the hidden layer(s)? We simply use the same equation but going backwards one layer. This algorithm is called <a href="[Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/chap2.html)" target="_blank"><em>back propagation</em></a> because it calculates the weights backwards from the final output.</p>

<p>But wait. We donâ€™t have the target output for the hidden layer. So how are we going to get the error for the hidden layer? We have to find another way.</p>

<h1 id="backpropagating-errors">Backpropagating errors</h1>

<p>If you think about it, the error at the output layer is contributed by the errors from the hidden layer according to the connections from the previous hidden layer. In other words, the combination of the errors for the hidden layer forms the errors for the output layer. And since the weights represent the importance of the input, it also represents the contribution of the error.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/error.png" width="400px" />
    
    
    <figcaption>
        <h4>Contribution of errors</h4>
        
    </figcaption>
    
</figure>


<p>As as result, we can use the ratio of the weights to calculate the change to make for each weight.  And because the denominator is constant, we can simplify this further by simply drop the denominators.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/error_backpropagate.png" width="500px" />
    
    
    <figcaption>
        <h4>Back propagating errors</h4>
        
    </figcaption>
    
</figure>


<p>Now letâ€™s see how we can back propagate the errors from the output layer using matrices.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/matrix2.png" width="700px" />
    
    
</figure>


<p>Once we have the errors from the hidden layer, we can use the same equation as before, but substituting the final output error with the hidden output error.</p>

<h1 id="learning-rate">Learning rate</h1>

<p>So an artificial neural network learns through back propagation using gradient descent. During gradient descent iterations itâ€™s often easy to overshoot, which results in the moving too quickly and stepping over the minimal <code>wjk</code>. To prevent that we use a <em>learning rate</em> <code>l</code> to scale down the amount we want to change for the weights.  This results in the change of our earlier equation:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/l.png" width="600px" />
    
    
</figure>


<p><code>l</code> is generally a small value so that we are more cautious about overshooting the minimum but we canâ€™t make it too small either, or else it will take too long to train.  There is quite a bit of research literature on setting the <a href="[Setting the learning rate of your neural network.](https://www.jeremyjordan.me/nn-learning-rate/)" target="_blank">best learning rate</a>.</p>

<h2 id="bias">Bias</h2>

<p>With our current neural network, the activation function is a sigmoid that cuts though <code>y</code> at 0.5. Any changes to the weights simply changes the the steepness of the sigmoid. As a result there is a limitation to how the neuron is being triggered. For example, to make the sigmoid return a low value of 0.1 when <code>x</code> is 2 is going to be impossible.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/nobias.png" width="600px" />
    
    
    <figcaption>
        <h4>Sigmoid functions without bias</h4>
        
    </figcaption>
    
</figure>


<p>However if we add a <a href="[Make Your Own Neural Network: Bias Nodes in Neural Networks](http://makeyourownneuralnetwork.blogspot.sg/2016/06/bias-nodes-in-neural-networks.html)" target="_blank"><em>bias</em></a> value to <code>x</code> this changes things altogether.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/bias.png" width="600px" />
    
    
    <figcaption>
        <h4>Sigmoid functions with bias</h4>
        
    </figcaption>
    
</figure>


<p>How we do this is by adding something called a <em>bias neuron</em> to the neural network. This bias neuron always outputs 1.0 and is added to a layer but doesnâ€™t have any have any input.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/ann_bias.png" width="600px" />
    
    
    <figcaption>
        <h4>Artificial neural network with bias</h4>
        
    </figcaption>
    
</figure>


<p>Not all neural networks need bias neurons. In the simple neural network weâ€™re going to write later, weâ€™ll not be using any bias neurons (and it works pretty ok).</p>

<h1 id="finally-some-code">Finally some code!</h1>

<p>So weâ€™re finally here! After all the concepts and maths, weâ€™re now going to start some implementation!</p>

<p>The code snippets in this post is not complete so don&rsquo;t simply cut and paste from here to run it. All the code here can be found in this Github repository:</p>

<p><a href="https://github.com/sausheong/gonn" target="_blank">https://github.com/sausheong/gonn</a></p>

<p>At this point in time Go doesnâ€™t have a lot of support in terms of libraries for machine learning, unlike Python. However there is a very useful library called <a href="[Gonum](https://www.gonum.org/)" target="_blank">Gonum</a> that provides what we need most â€” matrix manipulation.</p>

<p>Also, while Gonum has perfectly fine packages, I thought some of the quirks in Gonum makes it unnecessarily verbose so I created my own helper functions to overcome it.</p>

<h2 id="matrix-helpers">Matrix helpers</h2>

<p>Weâ€™ll start with the helper functions first. Gonumâ€™s main package for matrix manipulation is called <code>mat</code>. What weâ€™ll be using are primarily going to be the <code>mat.Matrix</code> interface and its implementation <code>mat.Dense</code>.</p>

<p>The <code>mat</code> package has a quirk that it requires us to create a new matrix with the exact correct rows and columns first, before we can execute the operations on the matrices. Doing so for multiple operations would be rather annoying so I wrapped around each function with my own.</p>

<p>For example, the Gonum <code>Product</code> function allows us to perform the dot product operation on two matrices, and I created a helper function that finds out the size of the matrix, creates it and perform the operation before returning the resultant matrix.</p>

<p>This helps to save around 1-3 lines of code or so, depending on the operation.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">_</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">n</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">Product</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span></code></pre></div>
<p>The <code>apply</code> function allows us to apply a function to the matrix.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">apply</span><span class="p">(</span><span class="nx">fn</span> <span class="kd">func</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="nx">j</span> <span class="kt">int</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">float64</span><span class="p">)</span> <span class="kt">float64</span><span class="p">,</span> <span class="nx">m</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">Apply</span><span class="p">(</span><span class="nx">fn</span><span class="p">,</span> <span class="nx">m</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span></code></pre></div>
<p>The <code>scale</code> function allows us to scale a matrix i.e. multiply a matrix by a scalar.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">scale</span><span class="p">(</span><span class="nx">s</span> <span class="kt">float64</span><span class="p">,</span> <span class="nx">m</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">Scale</span><span class="p">(</span><span class="nx">s</span><span class="p">,</span> <span class="nx">m</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span></code></pre></div>
<p>The <code>multiply</code> function multiplies 2 functions together (this is different from dot product`.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">multiply</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">MulElem</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span></code></pre></div>
<p>The <code>add</code> and <code>subtract</code> functions allow to add or subtract a function to/from another.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">add</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">Add</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span>

<span class="kd">func</span> <span class="nx">subtract</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="kc">nil</span><span class="p">)</span>
	<span class="nx">o</span><span class="p">.</span><span class="nx">Sub</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">o</span>
<span class="p">}</span></code></pre></div>
<p>Finally the <code>addScalar</code> function allows us to add a scalar value to each element in the matrix.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">addScalar</span><span class="p">(</span><span class="nx">i</span> <span class="kt">float64</span><span class="p">,</span> <span class="nx">m</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">a</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">r</span><span class="o">*</span><span class="nx">c</span><span class="p">)</span>
	<span class="k">for</span> <span class="nx">x</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">x</span> <span class="p">&lt;</span> <span class="nx">r</span><span class="o">*</span><span class="nx">c</span><span class="p">;</span> <span class="nx">x</span><span class="o">++</span> <span class="p">{</span>
		<span class="nx">a</span><span class="p">[</span><span class="nx">x</span><span class="p">]</span> <span class="p">=</span> <span class="nx">i</span>
	<span class="p">}</span>
	<span class="nx">n</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span><span class="p">,</span> <span class="nx">a</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">add</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">n</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<h2 id="the-neural-network">The neural network</h2>

<p>Here we go!</p>

<p>Weâ€™ll be creating a very simple 3 layer feedforward neural network (also known as multi-layer perceptron).  We start off with defining the network:</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">type</span> <span class="nx">Network</span> <span class="kd">struct</span> <span class="p">{</span>
	<span class="nx">inputs</span>        <span class="kt">int</span>
	<span class="nx">hiddens</span>       <span class="kt">int</span>
	<span class="nx">outputs</span>       <span class="kt">int</span>
	<span class="nx">hiddenWeights</span> <span class="o">*</span><span class="nx">mat</span><span class="p">.</span><span class="nx">Dense</span>
	<span class="nx">outputWeights</span> <span class="o">*</span><span class="nx">mat</span><span class="p">.</span><span class="nx">Dense</span>
	<span class="nx">learningRate</span>  <span class="kt">float64</span>
<span class="p">}</span></code></pre></div>
<p>The fields <code>inputs</code>, <code>hiddens</code> and <code>output</code> define the number of neurons in each of the input, hidden and output layers (remember, this is a 3 layer network). The <code>hiddenWeights</code> and <code>outputWeights</code> fields are matrices that represent the weights from the input to hidden layers, and the hidden to output layers respectively. Finally, the learningRate is, well, the learning rate for the network.</p>

<p>Next we have a simple method to actually create the neural network.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">CreateNetwork</span><span class="p">(</span><span class="nx">input</span><span class="p">,</span> <span class="nx">hidden</span><span class="p">,</span> <span class="nx">output</span> <span class="kt">int</span><span class="p">,</span> <span class="nx">rate</span> <span class="kt">float64</span><span class="p">)</span> <span class="p">(</span><span class="nx">net</span> <span class="nx">Network</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">net</span> <span class="p">=</span> <span class="nx">Network</span><span class="p">{</span>
		<span class="nx">inputs</span><span class="p">:</span>       <span class="nx">input</span><span class="p">,</span>
		<span class="nx">hiddens</span><span class="p">:</span>      <span class="nx">hidden</span><span class="p">,</span>
		<span class="nx">outputs</span><span class="p">:</span>      <span class="nx">output</span><span class="p">,</span>
		<span class="nx">learningRate</span><span class="p">:</span> <span class="nx">rate</span><span class="p">,</span>
	<span class="p">}</span>
	<span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span> <span class="p">=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddens</span><span class="p">,</span> <span class="nx">net</span><span class="p">.</span><span class="nx">inputs</span><span class="p">,</span> <span class="nx">randomArray</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">inputs</span><span class="o">*</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddens</span><span class="p">,</span> <span class="nb">float64</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">inputs</span><span class="p">)))</span>
	
	<span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span> <span class="p">=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputs</span><span class="p">,</span> <span class="nx">net</span><span class="p">.</span><span class="nx">hiddens</span><span class="p">,</span> <span class="nx">randomArray</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddens</span><span class="o">*</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputs</span><span class="p">,</span> <span class="nb">float64</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddens</span><span class="p">)))</span>
	<span class="k">return</span>
<span class="p">}</span></code></pre></div>
<p>The number of input, hidden and output neurons as well as the learning rate is passed in from the caller to create the network. However the hidden and output weights are randomly created.</p>

<p>If you remember from above, the weights we are creating is a matrix with the number of columns represented by <em>from</em> layer, and the number of rows represented by the <em>to</em> layer. This is because the number of rows in the weight must the same as the number of neurons in the <em>to</em> layer and the number of columns must be the same number of neurons as the <em>from</em> layer (in order to multiply with the outputs of the <em>from</em> layer). Take a while to look at the diagrams below again â€” it will make more sense.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/weights.png" width="600px" />
    
    
    <figcaption>
        <h4>Neural network and matrices</h4>
        
    </figcaption>
    
</figure>


<p>Initializing the weights with a random set of numbers is one of the important parameters. For this weâ€™re going to use a function <code>randomArray</code> to create this random array of float64.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">randomArray</span><span class="p">(</span><span class="nx">size</span> <span class="kt">int</span><span class="p">,</span> <span class="nx">v</span> <span class="kt">float64</span><span class="p">)</span> <span class="p">(</span><span class="nx">data</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">dist</span> <span class="o">:=</span> <span class="nx">distuv</span><span class="p">.</span><span class="nx">Uniform</span><span class="p">{</span>
		<span class="nx">Min</span><span class="p">:</span> <span class="o">-</span><span class="mi">1</span> <span class="o">/</span> <span class="nx">math</span><span class="p">.</span><span class="nx">Sqrt</span><span class="p">(</span><span class="nx">v</span><span class="p">),</span>
		<span class="nx">Max</span><span class="p">:</span> <span class="mi">1</span> <span class="o">/</span> <span class="nx">math</span><span class="p">.</span><span class="nx">Sqrt</span><span class="p">(</span><span class="nx">v</span><span class="p">),</span>
	<span class="p">}</span>

	<span class="nx">data</span> <span class="p">=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">size</span><span class="p">)</span>
	<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="p">&lt;</span> <span class="nx">size</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="nx">data</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="nx">dist</span><span class="p">.</span><span class="nx">Rand</span><span class="p">()</span>
	<span class="p">}</span>
	<span class="k">return</span>
<span class="p">}</span></code></pre></div>
<p>The <code>randomArray</code> function uses the <code>distuv</code> package in Gonum to create a uniformly distributed set of values between the range of <code>-1/sqrt(v)</code> and <code>1/sqrt(v)</code> where <code>v</code> is the size of the <em>from</em> layer. This is quite a commonly used distribution.</p>

<p>Now that we have our neural network, the two main functions we can ask it to do is to either train itself with a set of training data, or predict values given a set of test data.</p>

<p>From our hard work earlier on, we know that prediction means forward propagation through the network while training means forward propagation first, then back propagation later on to change the weights using some training data.</p>

<p>Since both training and prediction requires forward propagation, letâ€™s start with that first. We define a function called <code>Predict</code> to predict the values using the trained neural network.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="p">(</span><span class="nx">net</span> <span class="nx">Network</span><span class="p">)</span> <span class="nx">Predict</span><span class="p">(</span><span class="nx">inputData</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="c1">// forward propagation
</span><span class="c1"></span>	<span class="nx">inputs</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nx">inputData</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">inputData</span><span class="p">)</span>
	<span class="nx">hiddenInputs</span> <span class="o">:=</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">,</span> <span class="nx">inputs</span><span class="p">)</span>
	<span class="nx">hiddenOutputs</span> <span class="o">:=</span> <span class="nx">apply</span><span class="p">(</span><span class="nx">sigmoid</span><span class="p">,</span> <span class="nx">hiddenInputs</span><span class="p">)</span>
	<span class="nx">finalInputs</span> <span class="o">:=</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">,</span> <span class="nx">hiddenOutputs</span><span class="p">)</span>
	<span class="nx">finalOutputs</span> <span class="o">:=</span> <span class="nx">apply</span><span class="p">(</span><span class="nx">sigmoid</span><span class="p">,</span> <span class="nx">finalInputs</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">finalOutputs</span>
<span class="p">}</span></code></pre></div>
<p>We start off with the inputs first, by creating a matrix called <code>inputs</code> to represent the input values. Next we find the inputs to hidden layer by applying the dot product between the hidden weights and the inputs, creating a matrix called <code>hiddenInputs</code>. In other words, given a 2 neuron input layer and a 3 neuron hidden layer, this is what we get:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/matrix2.png" width="700px" />
    
    
    <figcaption>
        <h4>Using matrices with bias</h4>
        
    </figcaption>
    
</figure>


<p>Next, we apply our activation function, <code>sigmoid</code>  on the hidden inputs to produce <code>hiddenOutputs</code>.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">sigmoid</span><span class="p">(</span><span class="nx">r</span><span class="p">,</span> <span class="nx">c</span> <span class="kt">int</span><span class="p">,</span> <span class="nx">z</span> <span class="kt">float64</span><span class="p">)</span> <span class="kt">float64</span> <span class="p">{</span>
	<span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="nx">math</span><span class="p">.</span><span class="nx">Exp</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="o">*</span><span class="nx">z</span><span class="p">))</span>
<span class="p">}</span></code></pre></div>
<p>We repeat these 2 actions for final inputs and final outputs to produce <code>finalInputs</code> and <code>finalOutputs</code> respectively and the prediction is the final outputs.</p>

<p>So thatâ€™s how we predict using the forward propagation algorithm. Letâ€™s see how we do forward and back propagation in training.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="p">(</span><span class="nx">net</span> <span class="o">*</span><span class="nx">Network</span><span class="p">)</span> <span class="nx">Train</span><span class="p">(</span><span class="nx">inputData</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">targetData</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// forward propagation
</span><span class="c1"></span>	<span class="nx">inputs</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nx">inputData</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">inputData</span><span class="p">)</span>
	<span class="nx">hiddenInputs</span> <span class="o">:=</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">,</span> <span class="nx">inputs</span><span class="p">)</span>
	<span class="nx">hiddenOutputs</span> <span class="o">:=</span> <span class="nx">apply</span><span class="p">(</span><span class="nx">sigmoid</span><span class="p">,</span> <span class="nx">hiddenInputs</span><span class="p">)</span>
	<span class="nx">finalInputs</span> <span class="o">:=</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">,</span> <span class="nx">hiddenOutputs</span><span class="p">)</span>
	<span class="nx">finalOutputs</span> <span class="o">:=</span> <span class="nx">apply</span><span class="p">(</span><span class="nx">sigmoid</span><span class="p">,</span> <span class="nx">finalInputs</span><span class="p">)</span>

	<span class="c1">// find errors
</span><span class="c1"></span>	<span class="nx">targets</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="nx">targetData</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">targetData</span><span class="p">)</span>
	<span class="nx">outputErrors</span> <span class="o">:=</span> <span class="nx">subtract</span><span class="p">(</span><span class="nx">targets</span><span class="p">,</span> <span class="nx">finalOutputs</span><span class="p">)</span>
	<span class="nx">hiddenErrors</span> <span class="o">:=</span> <span class="nx">dot</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">.</span><span class="nx">T</span><span class="p">(),</span> <span class="nx">outputErrors</span><span class="p">)</span>

	<span class="c1">// backpropagate
</span><span class="c1"></span>	<span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span> <span class="p">=</span> <span class="nx">add</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">,</span>
		<span class="nx">scale</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">learningRate</span><span class="p">,</span>
			<span class="nx">dot</span><span class="p">(</span><span class="nx">multiply</span><span class="p">(</span><span class="nx">outputErrors</span><span class="p">,</span> <span class="nx">sigmoidPrime</span><span class="p">(</span><span class="nx">finalOutputs</span><span class="p">)),</span>
				<span class="nx">hiddenOutputs</span><span class="p">.</span><span class="nx">T</span><span class="p">()))).(</span><span class="o">*</span><span class="nx">mat</span><span class="p">.</span><span class="nx">Dense</span><span class="p">)</span>
	
	<span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span> <span class="p">=</span> <span class="nx">add</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">,</span>
		<span class="nx">scale</span><span class="p">(</span><span class="nx">net</span><span class="p">.</span><span class="nx">learningRate</span><span class="p">,</span>
			<span class="nx">dot</span><span class="p">(</span><span class="nx">multiply</span><span class="p">(</span><span class="nx">hiddenErrors</span><span class="p">,</span> <span class="nx">sigmoidPrime</span><span class="p">(</span><span class="nx">hiddenOutputs</span><span class="p">)),</span>
				<span class="nx">inputs</span><span class="p">.</span><span class="nx">T</span><span class="p">()))).(</span><span class="o">*</span><span class="nx">mat</span><span class="p">.</span><span class="nx">Dense</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<p>The forward propagation part is exactly the same as in the <code>Predict</code> function. We are not calling <code>Predict</code> here though because we still need the other intermediary values.</p>

<p>The first thing we need to do after getting the final outputs is to determine the output errors. This is relatively simple, we simply subtract our target data from the final outputs to get <code>outputErrors</code>:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/errork1.png" width="250px" />
    
    
</figure>


<p>The hidden errors from the hidden layer is a bit trickier. Remember this?</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/matrix1.png" width="600px" />
    
    
</figure>


<p>We use back propagation to calculate the hidden errors by applying the dot product on the transpose of the output weights and the output errors. This will give us <code>hiddenErrors</code>.</p>

<p>Now that we have the errors, we simply use the formula we derived earlier (including the learning rate) for changes to the weights we need to do:</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/l.png" width="600px" />
    
    
</figure>


<p>Remember that we are subtracting this number from the weights. Since this is a negative number, we end up adding this to the weights, which is what we did.</p>

<p>To simplify the calculations we use a <code>sigmoidPrime</code> function, which is nothing more than doing <code>sigP = sig(1 - sig)</code>:</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">sigmoidPrime</span><span class="p">(</span><span class="nx">m</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span><span class="p">)</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">Matrix</span> <span class="p">{</span>
	<span class="nx">rows</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">m</span><span class="p">.</span><span class="nx">Dims</span><span class="p">()</span>
	<span class="nx">o</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">rows</span><span class="p">)</span>
	<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">o</span> <span class="p">{</span>
		<span class="nx">o</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="mi">1</span>
	<span class="p">}</span>
	<span class="nx">ones</span> <span class="o">:=</span> <span class="nx">mat</span><span class="p">.</span><span class="nx">NewDense</span><span class="p">(</span><span class="nx">rows</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="nx">o</span><span class="p">)</span>
	<span class="k">return</span> <span class="nx">multiply</span><span class="p">(</span><span class="nx">m</span><span class="p">,</span> <span class="nx">subtract</span><span class="p">(</span><span class="nx">ones</span><span class="p">,</span> <span class="nx">m</span><span class="p">))</span> <span class="c1">// m * (1 - m)
</span><span class="c1"></span><span class="p">}</span></code></pre></div>
<p>Also you might see that weâ€™re doing the dot product of the transpose of the previous output â€” this is because weâ€™re multiplying across layers.</p>

<p>Finally we do this twice to get the new hidden and output weights for our neural network.</p>

<p>And thatâ€™s a wrap for the <code>Train</code> function.</p>

<h2 id="saving-the-training-results">Saving the training results</h2>

<p>Before we move on to using the neural network, weâ€™ll see how we can save our training results and load it up for use later. We certainly donâ€™t want to train from scratch each time we want to do the prediction â€” training the network can often take quite a while.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">save</span><span class="p">(</span><span class="nx">net</span> <span class="nx">Network</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">h</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="s">&#34;data/hweights.model&#34;</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">h</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">.</span><span class="nx">MarshalBinaryTo</span><span class="p">(</span><span class="nx">h</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="nx">o</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Create</span><span class="p">(</span><span class="s">&#34;data/oweights.model&#34;</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">o</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">.</span><span class="nx">MarshalBinaryTo</span><span class="p">(</span><span class="nx">o</span><span class="p">)</span>
	<span class="p">}</span>
<span class="p">}</span>

<span class="c1">// load a neural network from file
</span><span class="c1"></span><span class="kd">func</span> <span class="nx">load</span><span class="p">(</span><span class="nx">net</span> <span class="o">*</span><span class="nx">Network</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">h</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="s">&#34;data/hweights.model&#34;</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">h</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">.</span><span class="nx">Reset</span><span class="p">()</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">hiddenWeights</span><span class="p">.</span><span class="nx">UnmarshalBinaryFrom</span><span class="p">(</span><span class="nx">h</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="nx">o</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="s">&#34;data/oweights.model&#34;</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">o</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">.</span><span class="nx">Reset</span><span class="p">()</span>
		<span class="nx">net</span><span class="p">.</span><span class="nx">outputWeights</span><span class="p">.</span><span class="nx">UnmarshalBinaryFrom</span><span class="p">(</span><span class="nx">o</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="k">return</span>
<span class="p">}</span></code></pre></div>
<p>The <code>save</code> and <code>load</code> functions are mirror images of each other and we use a convenient function from the Gonum <code>mat</code> package to marshal the weight matrices into binary form and unmarshal the same form back to matrices. This is pretty mundane â€” the only thing of note is that when we unmarshal from the binary data back to the weight matrices, we need to first reset the matrices back to zero-value so that it can be reused.</p>

<h1 id="using-our-neural-network">Using our neural network</h1>

<p>Weâ€™re finally here â€” using the neural network!</p>

<h2 id="mnist-handwriting-recognition">MNIST handwriting recognition</h2>

<p>Letâ€™s start with a â€˜hello worldâ€™ of machine learning â€” using the MNIST dataset  to recognise handwritten numeric digits. The MNIST dataset is a set of 60,000 scanned handwritten digit images used for training and 10,000 similar images used for testing. Itâ€™s a subset of a larger set from NIST (National Institute of Standards and Technology)  that has been size-normalised and centered. The images are in black and white and are 28 x 28 pixels. The original dataset are stored in a format is that more difficult to work with, so people have come up with <a href="[MNIST in CSV](https://pjreddie.com/projects/mnist-in-csv/)" target="_blank">simpler CSV formatted datasets</a>, which is what weâ€™re using.</p>


<figure >
    
        <img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/mnist_dataset.png" width="700px" />
    
    
    <figcaption>
        <h4>MNIST dataset</h4>
        
    </figcaption>
    
</figure>


<p>In the CSV format every line is an image, and each column except the first represents a pixel. The first column is the label, which is the actual digit that the image is supposed to represent. In other words, this is the target output. Since there are 28 x 28 pixels, this means there are 785 columns in every row.</p>

<p>Letâ€™s start with the training. We create a function called <code>mnistTrain</code> that takes in a neural network and use it for training the MNIST dataset:</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">mnistTrain</span><span class="p">(</span><span class="nx">net</span> <span class="o">*</span><span class="nx">Network</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">rand</span><span class="p">.</span><span class="nx">Seed</span><span class="p">(</span><span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">().</span><span class="nx">UTC</span><span class="p">().</span><span class="nx">UnixNano</span><span class="p">())</span>
	<span class="nx">t1</span> <span class="o">:=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">()</span>

	<span class="k">for</span> <span class="nx">epochs</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">epochs</span> <span class="p">&lt;</span> <span class="mi">5</span><span class="p">;</span> <span class="nx">epochs</span><span class="o">++</span> <span class="p">{</span>
		<span class="nx">testFile</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="s">&#34;mnist_dataset/mnist_train.csv&#34;</span><span class="p">)</span>
		<span class="nx">r</span> <span class="o">:=</span> <span class="nx">csv</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">bufio</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">testFile</span><span class="p">))</span>
		<span class="k">for</span> <span class="p">{</span>
			<span class="nx">record</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">r</span><span class="p">.</span><span class="nx">Read</span><span class="p">()</span>
			<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="nx">io</span><span class="p">.</span><span class="nx">EOF</span> <span class="p">{</span>
				<span class="k">break</span>
			<span class="p">}</span>

			<span class="nx">inputs</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">net</span><span class="p">.</span><span class="nx">inputs</span><span class="p">)</span>
			<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">inputs</span> <span class="p">{</span>
				<span class="nx">x</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">strconv</span><span class="p">.</span><span class="nx">ParseFloat</span><span class="p">(</span><span class="nx">record</span><span class="p">[</span><span class="nx">i</span><span class="p">],</span> <span class="mi">64</span><span class="p">)</span>
				<span class="nx">inputs</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="p">(</span><span class="nx">x</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
			<span class="p">}</span>

			<span class="nx">targets</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
			<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">targets</span> <span class="p">{</span>
				<span class="nx">targets</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="mf">0.01</span>
			<span class="p">}</span>
			<span class="nx">x</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">strconv</span><span class="p">.</span><span class="nx">Atoi</span><span class="p">(</span><span class="nx">record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
			<span class="nx">targets</span><span class="p">[</span><span class="nx">x</span><span class="p">]</span> <span class="p">=</span> <span class="mf">0.99</span>

			<span class="nx">net</span><span class="p">.</span><span class="nx">Train</span><span class="p">(</span><span class="nx">inputs</span><span class="p">,</span> <span class="nx">targets</span><span class="p">)</span>
		<span class="p">}</span>
		<span class="nx">testFile</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="p">}</span>
	<span class="nx">elapsed</span> <span class="o">:=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Since</span><span class="p">(</span><span class="nx">t1</span><span class="p">)</span>
	<span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&#34;\nTime taken to train: %s\n&#34;</span><span class="p">,</span> <span class="nx">elapsed</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<p>We open up the CSV file and read each record, then process each record. For every record we read in we create an array that represents the inputs and an array that represents the targets.</p>

<p>For the <code>inputs</code> array we take each pixel from the record, and convert it to a value between 0.0 and 1.0 with 0.0 meaning a pixel with no value and 1.0 meaning a full pixel.</p>

<p>For the <code>targets</code> array, each element of the array represents the probability of the index being the target digit. For example, if the target digit is 3, then the 4th element <code>targets[3]</code> would have a probability of 0.99 while the rest would have a probability of 0.01.</p>

<p>Once we have the inputs and targets, we call the <code>Train</code> function of the network and pass it the inputs and targets.</p>

<p>You might notice that we ran this in â€˜epochsâ€™. Basically what we did was to run this multiple times because the more times we run through the training the better trained the neural network will be. However if we over-train it, the network will <em>overfit</em>, meaning it will adapt too well with the training data and will ultimately perform badly with data that it hasnâ€™t seen before.</p>

<p>Predicting the hand-written images is basically the same thing, except that we call the <code>Predict</code> function with only the inputs.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">mnistPredict</span><span class="p">(</span><span class="nx">net</span> <span class="o">*</span><span class="nx">Network</span><span class="p">)</span> <span class="p">{</span>
	<span class="nx">t1</span> <span class="o">:=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Now</span><span class="p">()</span>
	<span class="nx">checkFile</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="s">&#34;mnist_dataset/mnist_test.csv&#34;</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">checkFile</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>

	<span class="nx">score</span> <span class="o">:=</span> <span class="mi">0</span>
	<span class="nx">r</span> <span class="o">:=</span> <span class="nx">csv</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">bufio</span><span class="p">.</span><span class="nx">NewReader</span><span class="p">(</span><span class="nx">checkFile</span><span class="p">))</span>
	<span class="k">for</span> <span class="p">{</span>
		<span class="nx">record</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">r</span><span class="p">.</span><span class="nx">Read</span><span class="p">()</span>
		<span class="k">if</span> <span class="nx">err</span> <span class="o">==</span> <span class="nx">io</span><span class="p">.</span><span class="nx">EOF</span> <span class="p">{</span>
			<span class="k">break</span>
		<span class="p">}</span>
		<span class="nx">inputs</span> <span class="o">:=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nx">net</span><span class="p">.</span><span class="nx">inputs</span><span class="p">)</span>
		<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="k">range</span> <span class="nx">inputs</span> <span class="p">{</span>
			<span class="k">if</span> <span class="nx">i</span> <span class="o">==</span> <span class="mi">0</span> <span class="p">{</span>
				<span class="nx">inputs</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="mf">1.0</span>
			<span class="p">}</span>
			<span class="nx">x</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">strconv</span><span class="p">.</span><span class="nx">ParseFloat</span><span class="p">(</span><span class="nx">record</span><span class="p">[</span><span class="nx">i</span><span class="p">],</span> <span class="mi">64</span><span class="p">)</span>
			<span class="nx">inputs</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="p">(</span><span class="nx">x</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
		<span class="p">}</span>
		<span class="nx">outputs</span> <span class="o">:=</span> <span class="nx">net</span><span class="p">.</span><span class="nx">Predict</span><span class="p">(</span><span class="nx">inputs</span><span class="p">)</span>
		<span class="nx">best</span> <span class="o">:=</span> <span class="mi">0</span>
		<span class="nx">highest</span> <span class="o">:=</span> <span class="mf">0.0</span>
		<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="p">&lt;</span> <span class="nx">net</span><span class="p">.</span><span class="nx">outputs</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span> <span class="p">{</span>
			<span class="k">if</span> <span class="nx">outputs</span><span class="p">.</span><span class="nx">At</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">&gt;</span> <span class="nx">highest</span> <span class="p">{</span>
				<span class="nx">best</span> <span class="p">=</span> <span class="nx">i</span>
				<span class="nx">highest</span> <span class="p">=</span> <span class="nx">outputs</span><span class="p">.</span><span class="nx">At</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
			<span class="p">}</span>
		<span class="p">}</span>
		<span class="nx">target</span><span class="p">,</span> <span class="nx">_</span> <span class="o">:=</span> <span class="nx">strconv</span><span class="p">.</span><span class="nx">Atoi</span><span class="p">(</span><span class="nx">record</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
		<span class="k">if</span> <span class="nx">best</span> <span class="o">==</span> <span class="nx">target</span> <span class="p">{</span>
			<span class="nx">score</span><span class="o">++</span>
		<span class="p">}</span>
	<span class="p">}</span>

	<span class="nx">elapsed</span> <span class="o">:=</span> <span class="nx">time</span><span class="p">.</span><span class="nx">Since</span><span class="p">(</span><span class="nx">t1</span><span class="p">)</span>
	<span class="nx">fmt</span><span class="p">.</span><span class="nx">Printf</span><span class="p">(</span><span class="s">&#34;Time taken to check: %s\n&#34;</span><span class="p">,</span> <span class="nx">elapsed</span><span class="p">)</span>
	<span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">&#34;score:&#34;</span><span class="p">,</span> <span class="nx">score</span><span class="p">)</span>
<span class="p">}</span></code></pre></div>
<p>The results that we get is an array of probabilities. We figure out the element with the highest probability and the digit should be the index of that element. If it is, we count that as a win. The final count of the wins is our final score.</p>

<p>Because we have 10,000 test images, if we manage to detect all of them accurately then we have will 100% accuracy. Letâ€™s look at the <code>main</code> function:</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
	<span class="c1">// 784 inputs - 28 x 28 pixels, each pixel is an input
</span><span class="c1"></span>	<span class="c1">// 200 hidden neurons - an arbitrary number
</span><span class="c1"></span>	<span class="c1">// 10 outputs - digits 0 to 9
</span><span class="c1"></span>	<span class="c1">// 0.1 is the learning rate
</span><span class="c1"></span>	<span class="nx">net</span> <span class="o">:=</span> <span class="nx">CreateNetwork</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

	<span class="nx">mnist</span> <span class="o">:=</span> <span class="nx">flag</span><span class="p">.</span><span class="nx">String</span><span class="p">(</span><span class="s">&#34;mnist&#34;</span><span class="p">,</span> <span class="s">&#34;&#34;</span><span class="p">,</span> <span class="s">&#34;Either train or predict to evaluate neural network&#34;</span><span class="p">)</span>
	<span class="nx">flag</span><span class="p">.</span><span class="nx">Parse</span><span class="p">()</span>

	<span class="c1">// train or mass predict to determine the effectiveness of the trained network
</span><span class="c1"></span>	<span class="k">switch</span> <span class="o">*</span><span class="nx">mnist</span> <span class="p">{</span>
	<span class="k">case</span> <span class="s">&#34;train&#34;</span><span class="p">:</span>
		<span class="nx">mnistTrain</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
		<span class="nx">save</span><span class="p">(</span><span class="nx">net</span><span class="p">)</span>
	<span class="k">case</span> <span class="s">&#34;predict&#34;</span><span class="p">:</span>
		<span class="nx">load</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
		<span class="nx">mnistPredict</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
	<span class="k">default</span><span class="p">:</span>
		<span class="c1">// don&#39;t do anything
</span><span class="c1"></span>	<span class="p">}</span>
<span class="p">}</span></code></pre></div>
<p>This is pretty straightforward, we first create a neural network with 784 neurons in the input layer (each pixel is one input), 200 neurons in the hidden layer and 10 neurons in the output layer, one for each digit.</p>

<p>Then we train the network with the MNIST training dataset, and the predict the images with the testing dataset. This is what I have when I test it:</p>

<p><img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/mnist_screenshot.png" alt="" /></p>

<p>It took 8 mins to train the network with 60,000 images and 5 epochs, and 4.4 secs to test it with 10,000 images. The result is 9,772 images were predicted correctly, which is 97.72% accuracy!</p>

<h2 id="predicting-individual-files">Predicting individual files</h2>

<p>Now that we have tested our network, letâ€™s see how to use it on individual images.</p>

<p>First we get the data from the PNG file. To do this, we create a <code>dataFromImage</code> function.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">dataFromImage</span><span class="p">(</span><span class="nx">filePath</span> <span class="kt">string</span><span class="p">)</span> <span class="p">(</span><span class="nx">pixels</span> <span class="p">[]</span><span class="kt">float64</span><span class="p">)</span> <span class="p">{</span>
	<span class="c1">// read the file
</span><span class="c1"></span>	<span class="nx">imgFile</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">os</span><span class="p">.</span><span class="nx">Open</span><span class="p">(</span><span class="nx">filePath</span><span class="p">)</span>
	<span class="k">defer</span> <span class="nx">imgFile</span><span class="p">.</span><span class="nx">Close</span><span class="p">()</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">&#34;Cannot read file:&#34;</span><span class="p">,</span> <span class="nx">err</span><span class="p">)</span>
	<span class="p">}</span>
	<span class="nx">img</span><span class="p">,</span> <span class="nx">err</span> <span class="o">:=</span> <span class="nx">png</span><span class="p">.</span><span class="nx">Decode</span><span class="p">(</span><span class="nx">imgFile</span><span class="p">)</span>
	<span class="k">if</span> <span class="nx">err</span> <span class="o">!=</span> <span class="kc">nil</span> <span class="p">{</span>
		<span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">&#34;Cannot decode file:&#34;</span><span class="p">,</span> <span class="nx">err</span><span class="p">)</span>
	<span class="p">}</span>

	<span class="c1">// create a grayscale image
</span><span class="c1"></span>	<span class="nx">bounds</span> <span class="o">:=</span> <span class="nx">img</span><span class="p">.</span><span class="nx">Bounds</span><span class="p">()</span>
	<span class="nx">gray</span> <span class="o">:=</span> <span class="nx">image</span><span class="p">.</span><span class="nx">NewGray</span><span class="p">(</span><span class="nx">bounds</span><span class="p">)</span>

	<span class="k">for</span> <span class="nx">x</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">x</span> <span class="p">&lt;</span> <span class="nx">bounds</span><span class="p">.</span><span class="nx">Max</span><span class="p">.</span><span class="nx">X</span><span class="p">;</span> <span class="nx">x</span><span class="o">++</span> <span class="p">{</span>
		<span class="k">for</span> <span class="nx">y</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">y</span> <span class="p">&lt;</span> <span class="nx">bounds</span><span class="p">.</span><span class="nx">Max</span><span class="p">.</span><span class="nx">Y</span><span class="p">;</span> <span class="nx">y</span><span class="o">++</span> <span class="p">{</span>
			<span class="kd">var</span> <span class="nx">rgba</span> <span class="p">=</span> <span class="nx">img</span><span class="p">.</span><span class="nx">At</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span> <span class="nx">y</span><span class="p">)</span>
			<span class="nx">gray</span><span class="p">.</span><span class="nx">Set</span><span class="p">(</span><span class="nx">x</span><span class="p">,</span> <span class="nx">y</span><span class="p">,</span> <span class="nx">rgba</span><span class="p">)</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="c1">// make a pixel array
</span><span class="c1"></span>	<span class="nx">pixels</span> <span class="p">=</span> <span class="nb">make</span><span class="p">([]</span><span class="kt">float64</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nx">gray</span><span class="p">.</span><span class="nx">Pix</span><span class="p">))</span>
	<span class="c1">// populate the pixel array subtract Pix from 255 because 
</span><span class="c1"></span>	<span class="c1">// that&#39;s how the MNIST database was trained (in reverse)
</span><span class="c1"></span>	<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="p">&lt;</span> <span class="nb">len</span><span class="p">(</span><span class="nx">gray</span><span class="p">.</span><span class="nx">Pix</span><span class="p">);</span> <span class="nx">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="nx">pixels</span><span class="p">[</span><span class="nx">i</span><span class="p">]</span> <span class="p">=</span> <span class="p">(</span><span class="nb">float64</span><span class="p">(</span><span class="mi">255</span><span class="o">-</span><span class="nx">gray</span><span class="p">.</span><span class="nx">Pix</span><span class="p">[</span><span class="nx">i</span><span class="p">])</span> <span class="o">/</span> <span class="mf">255.0</span> <span class="o">*</span> <span class="mf">0.99</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.01</span>
	<span class="p">}</span>
	<span class="k">return</span>
<span class="p">}</span></code></pre></div>
<p>Each pixel in the image represents an value but we canâ€™t use the normal RGBA, instead we need an <code>image.Gray</code> . From the <code>image.Gray</code> struct we get the <code>Pix</code> value and translate it into a <code>float64</code> value instead. The MNIST image is white on black, so we need to subtract each pixel value from 255.</p>

<p>Once we have the pixel array, itâ€™s quite straightforward. We use a <code>predictFromImage</code> function that takes in the neural network and predicts the digit from an image file. The results are an array of probabilities where the index is the digit. What we need to do is to find the index and return it.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">predictFromImage</span><span class="p">(</span><span class="nx">net</span> <span class="nx">Network</span><span class="p">,</span> <span class="nx">path</span> <span class="kt">string</span><span class="p">)</span> <span class="kt">int</span> <span class="p">{</span>
	<span class="nx">input</span> <span class="o">:=</span> <span class="nx">dataFromImage</span><span class="p">(</span><span class="nx">path</span><span class="p">)</span>
	<span class="nx">output</span> <span class="o">:=</span> <span class="nx">net</span><span class="p">.</span><span class="nx">Predict</span><span class="p">(</span><span class="nx">input</span><span class="p">)</span>
	<span class="nx">matrixPrint</span><span class="p">(</span><span class="nx">output</span><span class="p">)</span>
	<span class="nx">best</span> <span class="o">:=</span> <span class="mi">0</span>
	<span class="nx">highest</span> <span class="o">:=</span> <span class="mf">0.0</span>
	<span class="k">for</span> <span class="nx">i</span> <span class="o">:=</span> <span class="mi">0</span><span class="p">;</span> <span class="nx">i</span> <span class="p">&lt;</span> <span class="nx">net</span><span class="p">.</span><span class="nx">outputs</span><span class="p">;</span> <span class="nx">i</span><span class="o">++</span> <span class="p">{</span>
		<span class="k">if</span> <span class="nx">output</span><span class="p">.</span><span class="nx">At</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="p">&gt;</span> <span class="nx">highest</span> <span class="p">{</span>
			<span class="nx">best</span> <span class="p">=</span> <span class="nx">i</span>
			<span class="nx">highest</span> <span class="p">=</span> <span class="nx">output</span><span class="p">.</span><span class="nx">At</span><span class="p">(</span><span class="nx">i</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
		<span class="p">}</span>
	<span class="p">}</span>
	<span class="k">return</span> <span class="nx">best</span>
<span class="p">}</span></code></pre></div>
<p>Finally from the <code>main</code> function we print the image and predict the digit from the image.</p>
<div class="highlight"><pre class="chroma"><code class="language-go" data-lang="go"><span class="kd">func</span> <span class="nx">main</span><span class="p">()</span> <span class="p">{</span>
	<span class="c1">// 784 inputs - 28 x 28 pixels, each pixel is an input
</span><span class="c1"></span>	<span class="c1">// 100 hidden nodes - an arbitrary number
</span><span class="c1"></span>	<span class="c1">// 10 outputs - digits 0 to 9
</span><span class="c1"></span>	<span class="c1">// 0.1 is the learning rate
</span><span class="c1"></span>	<span class="nx">net</span> <span class="o">:=</span> <span class="nx">CreateNetwork</span><span class="p">(</span><span class="mi">784</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">)</span>

	<span class="nx">mnist</span> <span class="o">:=</span> <span class="nx">flag</span><span class="p">.</span><span class="nx">String</span><span class="p">(</span><span class="s">&#34;mnist&#34;</span><span class="p">,</span> <span class="s">&#34;&#34;</span><span class="p">,</span> <span class="s">&#34;Either train or predict to evaluate neural network&#34;</span><span class="p">)</span>
	<span class="nx">file</span> <span class="o">:=</span> <span class="nx">flag</span><span class="p">.</span><span class="nx">String</span><span class="p">(</span><span class="s">&#34;file&#34;</span><span class="p">,</span> <span class="s">&#34;&#34;</span><span class="p">,</span> <span class="s">&#34;File name of 28 x 28 PNG file to evaluate&#34;</span><span class="p">)</span>
	<span class="nx">flag</span><span class="p">.</span><span class="nx">Parse</span><span class="p">()</span>

	<span class="c1">// train or mass predict to determine the effectiveness of the trained network
</span><span class="c1"></span>	<span class="k">switch</span> <span class="o">*</span><span class="nx">mnist</span> <span class="p">{</span>
	<span class="k">case</span> <span class="s">&#34;train&#34;</span><span class="p">:</span>
		<span class="nx">mnistTrain</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
		<span class="nx">save</span><span class="p">(</span><span class="nx">net</span><span class="p">)</span>
	<span class="k">case</span> <span class="s">&#34;predict&#34;</span><span class="p">:</span>
		<span class="nx">load</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
		<span class="nx">mnistPredict</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
	<span class="k">default</span><span class="p">:</span>
		<span class="c1">// don&#39;t do anything
</span><span class="c1"></span>	<span class="p">}</span>

	<span class="c1">// predict individual digit images
</span><span class="c1"></span>	<span class="k">if</span> <span class="o">*</span><span class="nx">file</span> <span class="o">!=</span> <span class="s">&#34;&#34;</span> <span class="p">{</span>
		<span class="c1">// print the image out nicely on the terminal
</span><span class="c1"></span>		<span class="nx">printImage</span><span class="p">(</span><span class="nx">getImage</span><span class="p">(</span><span class="o">*</span><span class="nx">file</span><span class="p">))</span>
		<span class="c1">// load the neural network from file
</span><span class="c1"></span>		<span class="nx">load</span><span class="p">(</span><span class="o">&amp;</span><span class="nx">net</span><span class="p">)</span>
		<span class="c1">// predict which number it is
</span><span class="c1"></span>		<span class="nx">fmt</span><span class="p">.</span><span class="nx">Println</span><span class="p">(</span><span class="s">&#34;prediction:&#34;</span><span class="p">,</span> <span class="nx">predictFromImage</span><span class="p">(</span><span class="nx">net</span><span class="p">,</span> <span class="o">*</span><span class="nx">file</span><span class="p">))</span>
	<span class="p">}</span>
<span class="p">}</span></code></pre></div>
<p>Assuming the network has already been trained, this is what we get.</p>

<p><img src="https://raw.githubusercontent.com/sausheong/gonn/master/imgs/predict_screenshot.png" alt="" /></p>

<p>And thatâ€™s it, we have written a simple 3-layer feedforward neural network from scratch using Go!</p>

<h1 id="references">References</h1>

<p>Here are some of the references for I took when writing this post and the code.</p>

<ul>
<li>Tariq Rashidâ€™s <a href="[Make Your Own Neural Network 1.0, Tariq Rashid, eBook - Amazon.com](https://www.amazon.com/Make-Your-Own-Neural-Network-ebook/dp/B01EER4Z4G)" target="_blank">Make Your Own Neural Network</a> is a great book to learn the basics of neural networks with its easy style of explanation</li>
<li>Michael Nielsenâ€™s <a href="[Neural networks and deep learning](http://neuralnetworksanddeeplearning.com/)" target="_blank">Neural Networks and Deep Learning</a> free online book is another amazing resource to learn the intricacies of building neural networks</li>
<li>Daniel Whitenack wrote a book on <em>Machine Learning With Go</em> and his post on <a href="[Building a Neural Net from Scratch in Go](http://www.datadan.io/building-a-neural-net-from-scratch-in-go/)" target="_blank">Building a Neural Net from Scratch in Go</a> is quite educational</li>
<li>Ujjwal Karnâ€™s data science blog has a nice <a href="[A Quick Introduction to Neural Networks â€“ the data science blog](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)" target="_blank">introductory post on neural networks</a></li>
</ul>

    </div>
    
  </div>
</section>

<section class="section">
  <div class="container">
    <aside><div id="disqus_thread"></div></aside>
    <script type="text/javascript">
      var disqus_shortname = 'space';
      (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
      })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></noscript>
  </div>
</section>


<section class="section">
  <div class="container has-text-centered">
    <p>&copy; <a href="https://github.com/sausheong">Chang Sau Sheong</a> 2018</p>
    
  </div>
</section>

</body>
</html>
